# MoDGS

## 1 Introduction

+ 之前的算法需要多视点或单相机有较大幅度运动来提供多视图一致性信息进行重建，当没有多视点或者单相机运动幅度较少时，方法失效
+ 在本文中，提出单目动态高斯泼溅（MoDGS），用于从动态场景中随意捕获的单目视频中渲染新颖的视图图像
+ MoDGS 通过采用单目深度估计方法解决弱多视图约束问题，该方法提供输入视频的先验深度信息以帮助 3D 重建
+ 但是仅仅使用单目深度图来监督渲染的深度图效果不佳
  + **问题一**：单目深度图仅仅提供每一帧的信息，但无助于及时关联两帧之间的 3D 点。因此，仍然很难学习准确的随时间变化的变形场
  + **问题二**：不同帧之间估计的深度值尺度不一致
+ 发现==直接初始化 3D 空间中的变形场极大地有助于后续 4D 表示的学习并提高渲染质量==
+ 尽管深度值值不一致，==但不同帧中不同像素的深度值的顺序是稳定的==，这促使我们提出序数深度损失

+ MoDGS 方法有效地仅使用 RAFT中的**噪声帧间流图**作为输入，无需强远程像素对应即可表现良好。

## 3 Proposed Method

+ 问题定义：给定一个随意捕获的单目视频，目标是从该视频生成新视角图像
+ 学习一组在**canonical space**中的高斯==$\{G_i \vert i=1, \cdots,N\}$==
+ 学习一个变形场 ==$\mathcal{T}_t:\mathbb{R}^3 \rightarrow \mathbb{R}^3$== 将规范空间里的高斯转换到一个具体的时间步$t$中
+ 然后，对于一个时间步$t$和一个相机姿态，使用splatting算法来渲染一张图像

### 总览

+ 将单目视频分为图片序列$\{I_t \vert t=1, \cdots,T\}$与对应的相机姿态
+ 将变形场表示为一个函数$x_t=\mathcal{T}_t(x)$，将一个规范场3D坐标$x \in \mathbb{R}^3$，映射到$t$时刻的3D空间的坐标$x_t\in \mathbb{R}^3$
+ 对于每张图像$I_t$，使用单目深度估计模型来估计深度图$D_t$
+ 使用流估计方法RAFT来估计任意两张图像$I_{t_i}$与$I_{t_j}$之间的2D光流$F_{t_i \to t_j}$
+ 然后，使用3D感知初始化来初始化变形场
+ 初始化完成后，使用渲染损失和顺序深度损失训练高斯和变形场

###　高斯和变形场

+ 规范空间里的高斯
  + 在规范空间中定义一组高斯，遵循原始3DGS定义3D 位置、尺度向量、旋转和具有球谐函数的颜色。请注意，这个规范空间并不明确对应于任何时间戳，而只是一个包含所有高斯的规范位置的虚拟空间。

+ 变形场
  + 变形场$T_t$使用Omnimotion和CadeX的设计，是一个可逆MLP网络
  + 所有的时间步$t$归一化到$[0,1]$，共享同一个MLP网络

+ 渲染
  + 在训练了规范空间中的高斯和变形场之后，我们将使用变形场将规范空间中的高斯函数变形到特定的时间步长$t$。然后，完全遵循 3D GS 中的泼溅技术，从任意视点渲染图像。

### 3D感知初始化

+ 动机

  + 当只有随意捕获的单目视频时，很难从SfM中获得用于初始化的初始稀疏点集

  + 尽管可以从第一帧的深度图初始化所有高斯，但我们表明这会导致次优结果

+ 深度尺度的初始化

  + 由于不同时间戳上的估计深度图会有不同的尺度，因此我们首先估计每个帧的粗略尺度以统一尺度。我们通过首先分割出视频上的静态区域，然后使用最小二乘拟合计算比例来实现这一点。
  + 静态区域可以通过
    + 对 2D 流进行阈值处理
    + 或使用 SAM2 等分割方法进行分割来确定
  + 然后，在这些静态区域上，我们将特定时间戳处的深度值重新投影到第一帧，并最小化投影深度和第一帧深度之间的差异，这使我们能够求解每一帧的比例。我们用计算出的比例校正所有深度图。下面，我们默认重用$D_t$来表示校正后的深度图

+ 变形场的初始化

  + 给定两个深度图$D_{t_i}$和$D_{t_j}$，以及2D流$F_{t_i \to t_j}$
  + 将2D流转为3D流$F_{t_i \to t_j}^{3D}$
    + 首先，将深度图转换为 3D 空间中的 3D 点
    + 然后，2D流实际上关联两组3D点

  + 使用3D流训练变形场$\mathcal{T}$

    + 一个$I_{t_i}$中的像素与一个3D点$x_{t_i}$相关，查询$F_{t_i \to t_j}^{3D}$来寻找在时间步$t_j$上的目标点$x_{t_j}$，然后训练$\mathcal{T}$中的MLP固定步数来初始化变形场，损失函数为

    $$
    \ell_{\text {init }}=\sum\left\|\mathcal{T}_{t_j} \circ \mathcal{T}_{t_i}^{-1}\left(x_{t_i}\right)-x_{t_j}\right\|^2
    $$

+ 初始化高斯

  + 获得初始化的变形场后，在规范空间中初始化一组 3D 高斯
    + 我们通过首先转换所有深度图以获得 3D 点
    + 然后，这些 3D 点向后变形到规范 3D 空间。这意味着我们将所有时间戳的所有深度点转换到规范空间，这会导致大量的点
    + 然后，我们使用预定义的体素大小均匀地对这些点进行下采样，以减少点数，并使用这些下采样的 3D 点在规范空间中的位置来初始化所有高斯函数

+ 

## 4 Experiments

### 4.1 评估协议

#### 数据集

+ 使用4个数据集进行测试
+ DyNeRF 6个场景 18-20个同步相机 10-30s的视频 a man working on a desktop
  + 使用相机0进行训练，相机5和6进行测试

#### 评估设置

+ 采用一个静态相机记录训练视频，从其他相机的视点进行渲染来评估

#### 公制缩放

+ 

#### Baseline 方法

+ 与4个方法以用随意捕获的单目视频来生成新视角图片的方式进行对比
+ NeRF-Based
  + HexPlane
    + HexPlane使用在3D空间和时间空间的6个特征平面来表示场景
    + 我们发现如果仅仅以单视点的单目视频作为输入，HexPlane不会产生合理的结果
    + 因此，除了输入单目视频外，我们使用另一个不同视点的视频来训练HexPlane
  + RoDynRF
    + 一个SOTA NeRF-based DVS方法
    + 也应用了单视点深度估计作为监督来进行3D动态表示
    + 使用和我们一样的GeoWizard单视点深度估计方法来训练
+ GS-Based
  + Deformable-GS
    + 也将变形场与DVS的一组规范高斯相关联
  + SC-GS
    + 学习一组关键点，并使用这些关键点的变形来混合任意3D点的变形。

### 4.3 消融实验

