# MODGS: DYNAMIC GAUSSIAN SPLATTING FROM  CASUALLY-CAPTURED MONOCULAR VIDEOS

## ABSTRACT

在本文中，我们提出了一种新的管线——MoDGS，用于从随手捕获的单目视频渲染动态场景的新视图。以往的单目动态 NeRF 或 Gaussian Splatting 方法高度依赖快速移动的输入相机来构建多视图一致性，因此当相机是静止或缓慢移动时，这些方法在重建动态场景方面往往表现不佳。为了解决这一具有挑战性的任务，MoDGS 利用最新的单视图深度估计方法来指导动态场景的学习。随后，我们提出了一种新颖的具备 3D 感知的初始化方法，用于学习合理的形变场，并设计了一种新的鲁棒深度损失来指导动态场景几何的学习。大量实验结果表明，MoDGS 能够仅基于随手捕获的单目视频生成高质量的动态场景新视图，其性能相较于现有最先进的方法有大幅提升。代码将会公开。

## 1 INTRODUCTION

新视图合成（Novel View Synthesis, NVS）是计算机图形学和计算机视觉领域的一项重要任务，对于增强现实或虚拟现实等下游应用具有极大的促进作用。近年来，得益于 NeRF（Mildenhall 等，2020）、Instant-NGP（Müller 等，2022）以及 Gaussian Splatting（Kerbl 等，2023）等技术的发展，在静态场景上进行新视图合成的质量得到了显著提升，尤其是在输入图像充足的情况下。然而，当只有一个单目视频时，对动态场景进行新视图合成依然是一项具有挑战性的任务。

动态视图合成（Dynamic View Synthesis, DVS）在近来出现的神经表示方法（Mildenhall 等，2020）与 Gaussian Splatting（Kerbl 等，2023）等技术的推动下取得了显著进展。现有的大多数 DVS 方法（Cao & Johnson, 2023；Yang 等，2023b）通常需要借助由密集同步相机拍摄的多视角视频，以实现较好的渲染质量。虽然已有一些工作可以利用单目视频来进行 DVS，但正如 DyCheck（Gao 等，2022）所指出的，这些方法要求单目视频中的相机进行极大幅度的移动（即在不同视角间“瞬移式”的相机运动），才能利用这种伪多视角视频所提供的多视角一致性来重建动态场景的 3D 几何。然而，在随手拍摄的视频中，这种大幅度相机移动极为少见，因为日常视频通常是平缓移动或甚至是静止的相机拍摄。当相机运动缓慢或静止时，多视角一致性约束会大大减弱，现有的这些 DVS 方法都无法生成高质量的新视角图像，如图 1 所示。

在本文中，我们介绍了单眼动态高溅射（MoDGS），以在动态场景中从随意捕获的单眼视频中渲染新颖的图像。 MoDGS通过采用单眼深度估计方法（Fu等，2024）来解决弱的多视限制问题，该方法在输入视频上提供了先前的深度信息以帮助3D重建。

然而，我们发现，仅仅在 DVS 中应用单视图深度估计器来监督渲染的深度图不足以实现高质量的新视图合成。首先，深度监督仅为每帧提供信息，但无助于在时间上关联两帧之间的 3D 点。因此，我们仍然难以学习准确的时间相关变形场。其次，估计的深度值在不同帧之间不一致。

为了从单目视频中学习稳健的变形场，我们提出了一种变形场的 3D 感知初始化方案。现有方法（Katsumata 等人，2023 年）仅依赖于 2D 流量估计的监督，这会产生质量较差的结果，并且缺乏足够的多视图一致性。我们发现直接在 3D 空间中初始化变形场对后续 4D 表示的学习有很大帮助，并提高了渲染质量，如图 1 所示。

为了更好地利用估计的深度图进行监督，我们提出了一种新的深度损失来解决不同帧之间估计深度值的尺度不一致问题。

先前的方法（Li et al., 2023b; Liu et al., 2023a）通过最小化归一化渲染深度和深度先验的 L2 距离，使用尺度不变深度损失来监督渲染的深度图，而最新的方法（Zhu et al., 2023）提出使用 Pearson 相关性损失来监督渲染的深度图，以减轻重建场景和估计的深度图之间的尺度模糊性。然而，不同帧的估计深度图甚至在归一化到相同尺度后也不一致。为了应对这些挑战，我们观察到尽管值不一致，但不同帧中不同像素的深度值的顺序是稳定的，这促使我们提出一种序数深度损失。这种新颖的序数深度损失使我们能够充分利用估计的深度图进行高质量的新颖视图合成。

为了证明 MoDGS 的有效性，我们在三个广泛使用的数据集上进行了实验，即 Nvdia（Yoon 等人，2020 年）数据集、DyNeRF（Li 等人，2022 年）数据集和 Davis（PontTuset 等人，2017 年）数据集。我们还展示了一个自收集的数据集的结果，该数据集包含来自互联网的单目野外视频。我们采用精确的单目 DVS 评估设置，该设置仅使用一个摄像机的视频作为输入，同时评估另一个摄像机的视频。结果表明，我们的方法大大优于以前的 DVS 方法，并且在随意拍摄的单目视频上实现了高质量的 NVS。

## 2 RELATED WORK

近年来，许多研究都集中在静态和动态场景中的新视图合成任务上。主要代表是神经辐射场 (Mildenhall et al., 2020) 和高斯溅射及其变体。在本文中，我们主要关注动态场景中的视图合成。

==**动态 NeRF**== 最近的动态 NeRF 方法大致可分为两类。1）通过以时间为条件的时变神经辐射场表示（Gao et al., 2021; Li et al., 2022; Park et al., 2023）。例如，Park et al. (2023) 通过插值按时间索引的特征向量提出了一个简单的时空辐射场。2）通过规范空间 NeRF 和变形场表示（Guo et al., 2023; Li et al., 2021; Park et al., 2021a;b; Pumarola et al., 2021; Tretschk et al., 2021; Xian et al., 2021）。例如，NSFF（Li 等人，2021）使用表示为 3D 密集矢量场的前向和后向流对动态分量进行建模；Nerfies（Park 等人，2021a）和 HyperNeRF（Park 等人，2021b）将场景动态建模为映射到规范空间的变形场。基于网格的 NeRF（Mu ̈ller 等人，2022；Sara Fridovich-Keil 和 Alex Yu 等人，2022；Chen 等人，2022）的最新进展表明，静态 NeRF 的训练可以显着加速。因此，一些动态 NeRF 工作利用这些基于网格或混合表示进行快速优化 (Guo et al., 2023; Cao & Johnson, 2023; Fang et al., 2022; Fridovich-Keil et al., 2023; Shao et al., 2023; Wang et al., 2023a;b; Song et al., 2023; You & Hou, 2023)。

==**动态高斯溅射**== 最近出现的 3D 高斯溅射 (3DGS) 证明了其由于其明确的点云表示而具有超快速实时渲染的功效。最近的后续研究将 3DGS 扩展为对动态 3D 场景进行建模。Luiten 等人 (2023) 通过从同步多视角视频中进行逐帧训练来跟踪动态 3D 高斯。Yang 等人 (2023b) 通过引入变形 MLP 网络来建模 3D 流，提出了一种可变形版本的 3DGS。Wu 等人 (2023) 和 Duisterhof 等人 (2023) 也引入了变形场，但使用了更有效的 Hexplane 表示 (Cao & Johnson, 2023)。Yang 等人 (2023a) 提出了一种具有 4D 高斯基元集合的动态表示，其中时间演化可以用 4D 球谐函数编码。Bae 等人（2024）使用每个高斯特征向量对运动进行编码。其他一些作品（Li et al.，2023a；Lin et al.，2023；Liang et al.，2023）也研究了如何有效地对具有不同基的高斯运动进行编码。为了有效地学习高斯运动，一些作品（Feng et al.，2023；Yu et al.，2023；Huang et al.，2024）采用将运动聚类在一起以获得紧凑的表示。

==**来自休闲单目视频的 DVS**== 如 DyCheck (Gao et al., 2022) 所示，许多用于基准测试的现有单目动态视图合成数据集，如 D-NeRF (Pumarola et al., 2021)、HyperNeRF (Park et al., 2021b) 和 Nerfies (Park et al., 2021a)，通常涉及帧之间的显着相机移动，但物体的动态运动较小。虽然这种捕获方式有助于多视图约束和动态 3D 建模，但它并不代表休闲的日常视频捕获。使用休闲视频时，这些方法的重建结果会受到质量下降的影响。一些工作使用单目休闲视频解决动态 3D 场景建模。DynIBaR (Li et al., 2023b) 通过聚合来自附近视图的特征来实现基于图像的长序列动态场景渲染，但其训练成本对于长时间的场景优化来说很高。Lee et al. （2023）提出了一种结合静态和动态元素的混合表示，可以加快训练和渲染速度，尽管它需要为动态组件添加额外的每帧掩码。RoDynRF（Liu et al.，2023a）通过同时估计 NeRF 和相机参数，专注于稳健的动态 NeRF 重建。DpDy（Wang et al.，2024a）通过使用 SDS 损失监督（Poole et al.，2022）微调扩散模型来提高质量，但它需要大量的计算资源。DG-Marbles（Stearns et al.，2024）等并行工作使用高斯弹珠和分层学习策略来优化表示。Shape-of-Motion（Wang et al.，2024b）和 Mosca（Lei et al.，2024）依赖于显式运动表示，并使用深度估计和视频跟踪先验初始化场景变形。然而，我们的 MoDGS 方法有效地仅使用来自 RAFT（Teed & Deng，2020）的噪声帧间流图作为输入，无需强长距离像素对应即可表现良好。

==**深度图中的序数关系**== 近年来，像素之间的序数关系得到了研究，特别是在单目深度估计领域。Zoran 等人（2015）提出使用三类分类网络来预测给定像素对的顺序关系。然后可以通过优化约束二次优化问题来提取深度。同样，Fu 等人（2018）将深度预测问题视为多类分类问题。Chen 等人（2016）进一步提出了一种排名损失来学习度量深度，如果真实关系相等，则鼓励深度之间的差异较小；否则鼓励差异较大。然后，Pavlakos 等人（2018）将这种可微排名损失扩展到人体姿势估计任务。然而，这些工作仅使用有限数量的深度顺序进行训练（Chen 等人（2016 年）中的一对和 Pavlakos 等人（2018 年）中的 17 对），导致对深度图的监督很粗糙。它们的排序损失作为深度监督的直接应用尚未被探索。此外，我们的序数深度将渲染的度量深度图作为输入，它们是浮点数的密集网格。我们的任务不同于以前的深度估计和姿势估计任务，我们在附录 A.10 中展示了序数深度损失和深度排序损失的比较。

## 3 PROPOSED METHOD

给定一个随手捕获的单目视频，我们的目标是从该视频合成新的视角图像。我们提出了 MoDGS，它通过学习一组高斯分布 $\{G_i \vert i=1, \cdots,N\}$ 在一个标准空间中，以及一个形变场 $\mathcal{T}_t:\mathbb{R}^3 \rightarrow \mathbb{R}^3$ 来将这些高斯分布变形到特定的时间戳 $t$。然后，对于时间戳 $t$ 和相机姿态，我们使用高斯点投影渲染图像。

==**概述**== 如图 2 所示，为了训练 MoDGS，我们将单目视频分割成一系列已知相机姿态的图像 $\{I_t \vert t=1, \cdots,T\}$。我们将形变场表示为一个函数 $x_t=\mathcal{T}_t(x)$，它将标准三维空间中的一个位置$x \in \mathbb{R}^3$ 映射到时间 t 时刻的一个位置$x_t \in \mathbb{R}^3$。对于每一帧图像 $I_t$，我们利用一个单视图深度估计器（Fu 等，2024）来估计每张图像的深度图 $D_t$，并使用光流估计方法 RAFT（Teed & Deng, 2020）来估计图像 $I_{t_i}$ 和 $I_{t_j}$之间的 2D 光流 $F_{t_i \to t_j}$，其中 $t_i$和$t_j$ 是两个不同的时间戳。然后，我们通过 3D 感知初始化方案初始化我们的形变场，如第 3.2 节所述。初始化后，我们使用渲染损失和第 3.3 节中介绍的新深度损失来训练我们的高斯分布和形变场。接下来，我们首先介绍 MoDGS 中高斯分布和渲染过程的定义。

### 3.1 GAUSSIANS AND DEFORMATION FIELDS

==**标准空间中的高斯分布**== 我们在标准空间中定义了一组高斯分布，沿用原始的 3D Gaussian Splatting（Kerbl 等，2023）方法来定义一个三维位置、一个尺度向量、一个旋转和一个通过球面调和函数表示的颜色。需要注意的是，这个标准空间并不明确对应任何时间戳，它仅是一个虚拟空间，其中包含了所有高斯分布的标准位置。

==**形变场**== MoDGS 中使用的形变场$\mathcal{T}_t$ 参考了 Omnimotion（Wang 等，2023c）和 CaDeX（Lei & Daniilidis，2022）的设计，它是一个可逆的 MLP 网络（Dinh 等，2016）。这个可逆的 MLP 网络意味着 $\mathcal{T}_t$和 $\mathcal{T}_t^{-1}$ 都可以直接通过该 MLP 网络计算得到。不同时间戳 $t$ 对应的所有 $\mathcal{T}_t$ 共享同一个 MLP 网络，并且时间 $t $被归一化到 $[0, 1]$，作为输入传递给该 MLP 网络。

==**使用 MoDGS 渲染**== 在训练完标准空间中的高斯分布和形变场之后，我们将使用形变场将标准空间中的高斯分布变形到特定的时间步 t。然后，我们完全遵循 3D Gaussian Splatting（Kerbl 等，2023）中的高斯点投影技术，从任意视角渲染图像。

### 3.2 3D-AWARE INITIALIZATION

原始的 3D 高斯点投影（Kerbl 等，2023）依赖于来自Structure-from-Motion（SfM）的稀疏点来初始化所有高斯分布的位置。当我们只有一个随手捕获的单目视频时，从 SfM 中获取初始的稀疏点集合用于初始化是很困难的。尽管可以通过估计的单视图深度图来初始化第一个帧中的所有高斯分布，但我们证明这会导致次优的结果。同时，我们不仅需要初始化高斯分布，还需要初始化形变场。因此，我们提出了一个针对 MoDGS 的 3D 感知初始化方案。

==**深度尺度初始化**== 由于不同时间戳的估计深度图可能具有不同的尺度，我们首先为每一帧估计一个粗略的尺度，以统一各帧的尺度。我们通过首先在视频中分割出静态区域，然后使用最小二乘拟合（Chung 等，2023b）来计算尺度来实现这一目标。静态区域可以通过对 2D 光流（Teed & Deng，2020）进行阈值化处理，或者使用像 SAM2（Ravi 等，2024）这样的分割方法进行分割来确定。

接下来，在这些静态区域上，我们将特定时间戳的深度值重新投影到第一帧，并最小化投影深度与第一帧深度之间的差异，这使我们能够为每一帧求解一个尺度。我们用计算得到的尺度对所有深度图进行矫正。接下来，我们默认使用 $D_t$ 来表示矫正后的深度图。

==**形变场初始化**== 如图 3（左）所示，给定两个深度图 $D_{t_i}$ 和 $D_{t_j}$以及 2D 光流 $F_{t_i \to t_j}$，我们将它们提升为 3D 光流 $F_{t_i \to t_j}^{3D}$。具体来说，通过首先将深度图转换为 3D 空间中的 3D 点来实现这一点。然后，估计的 2D 光流 $F_{t_i \to t_j}$ 实际上将两组 3D 点关联起来，从而产生一个 3D 光流 $F_{t_i \to t_j}^{3D}$。得到这个 3D 光流后，我们将用它来训练我们的变形场 $\mathcal{T}$。具体而言，对于图像 $I_{t_i}$ 中的一个像素，其对应的 3D 点是 $x_{t_i}$，我们查询$F_{t_i \to t_j}^{3D}$ 来找到其在 $t_j$ 时间戳下的目标点  $x_{t_j}$。然后，我们通过最小化两者之间的差异来进行优化
$$
\ell_{\text{init}} = \sum \left\| \mathcal{T}_{t_j} \circ \mathcal{T}_{t_i}^{-1}(x_{t_i}) - x_{t_j} \right\|^2.
$$
我们在$\mathcal{T}$中训练MLP进行固定数量的步骤，以初始化变形场。

==**高斯初始化**== 在获得初始化的变形场后，我们将在规范空间中初始化一组 3D 高斯，如图 3（右）所示。我们通过以下步骤实现这一点：首先将所有深度图转换为 3D 点。然后，这些 3D 点被向后变形到规范 3D 空间。这意味着我们将所有时间戳的深度点转换到规范空间，这会导致大量的点。然后，我们使用预定义的体素大小均匀地对这些点进行下采样，以减少点的数量，并且我们用这些下采样后的 3D 点在规范空间中的位置初始化所有高斯。

### 3.3 ORDINAL DEPTH LOSS

==**皮尔逊相关损失**== 现有的动态高斯溅射或 NeRF 方法也采用深度损失来监督其 3D 表示的学习。一种可能的解决方案（Zhu et al., 2023; Li et al., 2021; Liu et al., 2023a）是最大化渲染深度与估计的单视角深度之间的皮尔逊相关性，
$$
\text{Corr}\left(\hat{D}_t, D_t\right) = \frac{\text{Cov}\left(\hat{D}_t, D_t\right)}{\sqrt{\text{Var}\left(\hat{D}_t\right) \text{Var}\left(D_t\right)}}
$$
其中 $ \text{Cov}(·, ·) $和 $\text{Var} (·, ·)$ 分别表示协方差和方差，$D_t$ 和 $\hat{D}_t$ 分别是估计的深度和渲染的深度。由于估计的单视角深度在尺度上存在模糊性，皮尔逊相关损失避免了尺度模糊性的负面影响。需要注意的是，在 Li et al. (2021) 和 Liu et al. (2023a) 中，该损失被称为标准化深度损失，而在这里，它等同于皮尔逊相关性，具体如补充材料中所示。

==**皮尔逊相关损失的局限性**== 然而，我们发现这种皮尔逊相关深度损失仍然是次优的。如图 4 所示，经过归一化后，在两个不同时间戳下估计的深度图仍然不一致。使两个深度图在归一化后保持一致，实际上要求这两个深度图之间存在一个线性变换关系，即 $D_{t+1} = aD_t + b$，其中 $a$ 和 $b$ 是两个常数。然而，单视角深度估计方法并不够准确，无法保证不同时间戳下两个估计的深度图之间存在线性关系。在这种情况下，皮尔逊相关损失仍然带来了不一致的监督。

==**顺序深度损失**== 为了解决这个问题，我们的观察是，尽管我们无法保证归一化后深度的一致性，如图 4 所示，两个不同帧之间的深度值顺序是保持一致的。因此，这激发了我们通过一种新的顺序深度损失来确保深度顺序的正确性。我们首先定义一个顺序指示函数
$$
\mathcal{R}\big(D_t(u_1), D_t(u_2)\big) =
\begin{cases}
+1, & D_t(u_1) > D_t(u_2) \\
-1, & D_t(u_1) < D_t(u_2)
\end{cases},
$$
其中，$\mathcal{R}$ 是深度图 $D_t$ 上的顺序指示函数，用于表示像素 $u_1 \in \mathbb{R}^2$ 和 $u_2 \in \mathbb{R}^2$ 之间的深度顺序，且 $D_t(u)$ 表示像素 $u$ 上的深度值。接下来，我们基于深度顺序定义我们的顺序深度损失，形式如下：
$$
\ell_{\text{ordinal}} = \left\| \tanh \left( \alpha ( \hat{D}_t(u_1) - \hat{D}_t(u_2) ) \right) - \mathcal{R} \left( D_t(u_1), D_t(u_2) \right) \right\|,
$$
其中，$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$，$\hat{D}_t$表示时间戳 $t$ 处的渲染深度图，$\hat{D}_t(u)$ 是该渲染深度图上像素 $u$ 的深度值，$\alpha$ 是一个预定义常数。公式 (3) 表示我们通过 tanh 函数将 $\hat{D}_t(u_1)$ 和 $\hat{D}_t(u_2)$ 之间的深度差异转换为 1 或 -1。接着，我们强制要求渲染深度图 $\hat{D}_t$ 的深度顺序与预测深度图 $D_t$ 中的顺序保持一致。在实现中，我们随机采样 100k 对 $(u_1, u_2)$ 来计算顺序深度损失。

### 3.4 TRAINING OF MODGS

在初始化高斯和变形场之后，我们使用 MoDGS 在特定时间戳进行渲染，并计算渲染损失 ${\ell_{render}}$ 和顺序深度损失 ${\ell_{ordinal}}$。因此，MoDGS 的总训练损失为：
$$
\ell = \lambda_{\text{ordinal}} \ell_{\text{ordinal}} + \lambda_{\text{render}} \ell_{\text{render}}.
$$

##  4 EXPERIMENTS

### 4.1 EVALUATION PROTOCOLS

==**数据集**== 为了验证我们方法的有效性，我们在四个数据集上进行了实验。第一个数据集是 DyNeRF (Li et al., 2022) 数据集，包含 6 个场景。每个场景中，我们有 18-20 个同步摄像头，拍摄 10-30 秒的视频。这些视频主要展示一名男子在桌面上工作，例如切牛肉或倒水。我们使用 camera0 进行训练，并在 camera5 和 camera6 上评估结果。第二个数据集是 Nvidia (Yoon et al., 2020) 数据集，包含更多种类的动态主体，如跳跃、玩气球和打开雨伞。Nvidia 数据集包含 8 个场景，也有 12 个同步摄像头。我们在 camera4 上训练所有方法，并在 camera3 和 camera5 上评估。此外，我们还收集了 6 个在线视频，构建了一个野外数据集，称为单目休闲视频 (MCV) 数据集，以展示我们的方法可以推广到野外的休闲视频。MCV 数据集包含多种不同的主体，如滑冰、狗吃食物、瑜伽等。MCV 数据集每个场景只包含一个视频，因此我们无法评估定量结果，只能报告该数据集上的定性结果。我们还在附录 A.6 中展示了 Davis 数据集 (Pont-Tuset et al., 2017) 的结果。

==**评估设置**== 以往的 DVS 方法（Cao & Johnson, 2023；Yang et al., 2023b；Gao et al., 2021）都使用不同的摄像头来训练动态 NeRF 或高斯 Splatting。尽管它们在特定时间戳只使用一个摄像头，但在不同时间戳使用不同的摄像头，以便构建伪多视角视频来学习场景的 3D 结构。由于我们的目标是对随意捕获的图像进行新视角合成，因此我们没有采用这种“传送摄像头运动”来构建训练视频，而是仅使用一个静态摄像头来录制训练视频。然后，我们从另一个摄像头的视点渲染图像进行评估。

==**评估指标**== 为了评估渲染质量，我们需要从新的视点渲染图像，并与真实图像进行比较。然而，如果输入视频几乎是静态的，则输入视频将包含不足的 3D 信息，并且可能存在尺度模糊。因此，不同的 DVS 方法在重建动态场景时会选择不同的尺度，使得在新视点上的渲染图像与给定的真实图像不对齐。为了解决这个问题，我们手动标注了训练图像和真实新视角图像之间的对应关系。然后，我们使用重建的动态场景在训练图像上渲染深度图，并优化尺度因子，以将深度值缩放至满足这些标注的对应关系。通过对齐不同方法的尺度因子与真实图像后，我们计算渲染图像与真实图像之间的 SSIM、LPIPS 和 PSNR。

==**基准方法**== 为了展示 MoDGS 在使用随意捕获的单目视频合成新视角图像方面的优越能力，我们将 MoDGS 与 4 个基准方法进行比较。这些方法可以分为两类。第一类是基于 NeRF 的方法，包括 HexPlane（Cao & Johnson, 2023）和 RoDynRF（Liu et al., 2023a）。HexPlane 用六个特征平面在 3D 空间和时间空间中表示场景。我们发现，如果仅给定单个摄像头的单目视频作为输入，HexPlane 不会产生合理的结果。因此，除了输入的单目视频外，我们还使用来自不同视点的另一段视频来训练 HexPlane 进行 DVS 任务。RoDynRF 是一种基于 NeRF 的最先进的 DVS 方法，它也采用单视角深度估计作为 3D 动态表示的监督。我们使用与我们相同的单视角深度估计器 GeoWizard（Fu et al., 2024）进行训练。第二类是基于高斯 Splatting 的 DVS 方法，包括 Deformable-GS（Yang et al., 2023b）和 SC-GS（Huang et al., 2024）。Deformable-GS 也将变形场与一组规范高斯关联，用于 DVS。SC-GS 学习一组关键点，并利用这些关键点的变形来混合任意 3D 点的变形。

### 4.2 COMPARISON WITH BASELINES

DyNeRF 和 Nvidia 数据集上的定性结果如图 5 所示。我们在 MCV 数据集上的其他定性结果见图 6。Nvidia 和 DyNeRF 数据集上的定量结果如表 1 所示。补充视频包含更多的比较结果。

从随意捕获的单目视频合成新视角是一个具有挑战性的任务。如图 5 所示，尽管基准方法在这些基准测试中通过“传送摄像头运动”取得了令人印象深刻的结果，这些方法仍然未能正确重建动态场景的 3D 几何形状，并在动态前景和静态背景中产生明显的伪影。其主要原因是单目摄像头几乎是静态的，并未提供足够的多视角一致性来重建高质量的 3D 几何形状以进行新视角合成。在图 5 的第三行中，SCGS（Huang et al., 2024）未能重建动态前景主体，因为 SC-GS 具有一个初始化过程，将整个场景视为静态场景，并在场景上训练若干步。当前景主体在大幅度运动（例如从左到右滑冰）时，它会被静态场景初始化忽略，然后我们在后续步骤中无法进行重建。

相比之下，我们的方法依赖于 3D 感知初始化，为后续优化提供了强有力的基础。同时，我们的顺序深度损失使得来自单视角深度估计器的 3D 先验能够精确重建动态场景。表 1 中的定量结果也表明，我们的方法在两个数据集的所有指标上都达到了最佳性能。需要注意的是，仍然在遮挡边界上存在一些伪影，因为输入的单目摄像头几乎是静态的，这些区域在我们的输入视频中不可见。

### 4.3 ABLATION STUDIES

我们在 DyNeRF (Li et al., 2022) 数据集上进行消融实验，验证初始化方法和深度损失的有效性。定性结果如图 7 和图 8 所示，定量结果如表 2 所示。在附录中，我们还提供了与深度扭曲的比较、对深度噪声的鲁棒性、视频深度的讨论、与其他深度排序损失的比较以及关于 alpha 值的讨论。

#### 4.3.1 3D-AWARE INITIALIZATION

为了展示我们 3D 感知初始化的有效性，我们采用了随机初始化作为变形场的初始值。在随机初始化的基础上，我们仍然将所有深度点向后变形到规范空间，并对这些点进行下采样以初始化高斯。然后，我们遵循完全相同的训练过程，训练这个随机初始化的基准方法。随机初始化的最终结果如图 7 所示。与我们的 3D 感知初始化相比，这种随机初始化在动态前景人物上产生了更多伪影，证明我们的 3D 感知初始化为后续的 3D 动态场景重建提供了良好的初始点。

#### 4.3.2 ORDINAL DEPTH LOSS

为了展示我们提出的顺序深度损失的有效性，我们在两个基准设置下进行实验：去除深度损失和使用皮尔逊相关作为深度损失。如图 8 所示，当没有深度损失时，重建的 3D 几何形状包含大量噪声，场景中存在明显的伪影。由于我们在这个实验中进行 3D 感知初始化以确保公平比较，尽管没有深度损失，结果仍然显示出合理的渲染深度图。采用皮尔逊深度损失通过线性相关渲染深度图和输入深度图提高了质量，但仍然在某些区域内产生噪声。相比之下，我们的顺序深度损失能够平滑重建深度图内部的区域，同时保持边界的锐利边缘。因此，所提出的顺序深度损失实现了更强鲁棒性的动态场景重建。

### 4.4 LIMITATIONS

尽管我们的方法能够从随意捕获的单目视频中进行动态视图合成，但这一任务仍然非常具有挑战性。一个限制是，我们的方法只能重建可见的 3D 部分，而无法想象未见部分，这在渲染这些未见部分的新视角视频时会导致伪影。结合最近的 3D 相关扩散生成模型（Chung et al., 2023a；Liu et al., 2023b；Long et al., 2023）可能是解决这个问题的一个有前景的方向，我们将在未来的工作中探索。另一个限制是，当前的训练时间与现有的 DVS 方法相当，单个场景可能需要几个小时。如何高效地重建动态场景将是一个有趣且有前景的未来研究课题。同时，当摄像头完全静止时，我们的方法强烈依赖于单视角深度估计器来估算 3D 深度图。尽管现有的单视角深度估计器（Fu et al., 2024；Ke et al., 2023；Yang et al., 2024；Bhat et al., 2023）已经在大规模数据集上进行训练，并能在大多数情况下预测合理的深度图，但这些深度估计器可能无法捕捉一些细节，从而降低质量。同时，镜面反射在单目设置中无法稳健且良好地处理，这也是一个极具挑战性的问题，我们将在未来的工作中继续探索。

## 5 CONCLUSION

在本文中，我们提出了一种新型的动态视图合成范式，称为 MoDGS。与现有的 DVS 方法需要“传送摄像头运动”不同，MoDGS 旨在从随意捕获的单目视频中渲染新视角图像。MoDGS 引入了两个新的设计来完成这一具有挑战性的任务。首先，提出了一种新的 3D 感知初始化方案，直接初始化变形场，为后续的优化提供合理的起点。我们进一步分析了深度损失的问题，并提出了一种新的顺序深度损失来监督场景几何的学习。在三个数据集上的大量实验表明，我们的方法在野外单目视频上的表现优于基准方法。